# Task 2
## outputs of kubectl get
```bash
[kinjalik@kinjalik-laptop k8s]$ kubectl get po,sts,svc,pvc
NAME               READY   STATUS    RESTARTS   AGE
pod/python-app-0   1/1     Running   0          2m44s
pod/python-app-1   1/1     Running   0          2m34s

NAME                          READY   AGE
statefulset.apps/python-app   2/2     2m44s

NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP          7d
service/python-app   NodePort    10.108.64.24   <none>        8000:31987/TCP   2m44s

NAME                                                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/counter-data-py-python-app-0   Bound    pvc-fc3786bf-42cd-4f39-b897-23ecc6a5fa3e   10Mi       RWO            standard       13m
persistentvolumeclaim/counter-data-py-python-app-1   Bound    pvc-0fe52353-5504-4b75-b161-db51ba46c5cd   10Mi       RWO            standard       12m
```

## Values in volumes
```bash
[kinjalik@kinjalik-laptop k8s]$ kubectl exec python-app-0 -- cat /data/counter.json
{"counter": 39}
[kinjalik@kinjalik-laptop k8s]$ kubectl exec python-app-1 -- cat /data/counter.json
{"counter": 16}
```

These values are different because each pod has its own persistant storage. However, these files will persist across pod updates

## Why ordering guarantees not necessary?
There are no dependencies between pods of the same kind. Ordering would be needed in case we have separation (e.g., first pod is responsible for reading, second for writing).

## Parallel operations implementation
This task was complete using `podManagementPolicy`

## Update strategies
1. Rolling strategy --- update set of replicas pod-by-pod: stop first pod, delete it, create updated pod, start it, then rewind and repeat with each old pod. If there is more than 1 replica, then it will cause no downtime and slight performance reduction. It is default strategy
2. Recreate strategy --- update everything at once: stop all pods, delete them, create new pods, start them. It will cause downtime so it's not recommended to use it
3. Canary update strategy --- firstly, create several pods with new version (25% more pods). Then, if everything fine with them, upgrade old pods to new and delete canary. It provides an ability to have some time to detect the bug on portion of real users. In fact, it's not a really feature of k8s, but just an approach of deployments organization.